{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4190e1-30c0-4388-bcd4-c487ff0187b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3095906-ce6a-4cdf-9832-6675dcc9fefd",
   "metadata": {},
   "source": [
    "This notebook is based on a [tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb) from [Ben Trevett](https://github.com/bentrevett) and uses the IMDB Movie Reviews dataset. This is a set of movie reviews that are labelled as either positive or negative. The goal of the problem is to correctly infer the sentiment of the review from the text. The main changes from the source tutorial are that we try and mirror the style of approach taken in the [AG News Classification notebook](https://github.com/DavidEdwards1/pytorch-text/blob/main/notebooks/AG%20News%20Classification.ipynb) when it comes to creating the text processing pipeline and collator etc.\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "Our text processing pipeline is similar to that used in the [AG News Classification notebook](https://github.com/DavidEdwards1/pytorch-text/blob/main/notebooks/AG%20News%20Classification.ipynb) where we first tokenise the text and then create a vocabulary that will encode each token as an integer. The main \"upgrade\" here is that we use [SpaCy](https://spacy.io/) to tokenise the text (specifically their [`en_core_web_md`](https://spacy.io/models/en#en_core_web_md) model).A question worth investigating could be: does it actually make a difference over using `torchtext.data.utils.get_tokenizer('basic_english')` tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360e03cc-402b-4508-9f73-340d762348ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "counter = collections.Counter()\n",
    "for (label, text) in train_iter:\n",
    "    counter.update((t.text for t in tokenizer(text)))\n",
    "    \n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66577c16-e764-4ae5-8609-d1e706738bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1026, 9, 43, 491]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token.text] for token in tokenizer(\"Here is an example\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994d8521-c0c5-4fa2-8b35-d7351d236509",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class TextPipeline:\n",
    "    def __init__(self, vocab, tokenizer):\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        return [self.vocab[token.text] for token in self.tokenizer(text)]\n",
    "\n",
    "class LabelPipeline:\n",
    "    def __call__(self, label):\n",
    "        return 0 if label == \"neg\" else 1\n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, text_pipeline, label_pipeline):\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare batch of data to be used as input to torch model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          labels: a torch.tensor of integer encoded labels. Has shape (batch_size)\n",
    "          texts: a torch.tensor of integer encoded text sequences. Encoded using text_pipeline.\n",
    "              Each example is concatenated together into a flat 1D tensor. The start of each\n",
    "              example is recorded in offsets. Has shape (n_tokens_in_batch)\n",
    "          offsets: a torch.tensor of the index of the start of each example.\n",
    "              Has shape (batch_size)\n",
    "        \"\"\"\n",
    "        labels, texts, offsets = [], [], [0]\n",
    "\n",
    "        for (label, text) in batch:\n",
    "            labels.append(\n",
    "                self.label_pipeline(label)\n",
    "            )\n",
    "            processed_text = torch.tensor(\n",
    "                self.text_pipeline(text),\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "            texts.append(processed_text)\n",
    "            offsets.append(processed_text.size(0)) # length of processed text\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # starting index of each example\n",
    "        texts = torch.cat(texts) # we can treat this differently as it is a list of tensors\n",
    "\n",
    "        return labels.to(device), texts.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe93170-d9dd-484f-b59c-75319a6c2328",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = IMDB(split='train')\n",
    "\n",
    "collator = Collator(\n",
    "    TextPipeline(vocab, tokenizer),\n",
    "    LabelPipeline()\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04006350-89e8-41dc-ae8b-5f60e3d4a101",
   "metadata": {},
   "source": [
    "# Predictive Model\n",
    "\n",
    "Fairly simple RNN model. We pass the tokenized and numericalised text into an Embedding Layer. Then to a recursive layer. Essentially each vector from an input gets passed through the recursive layer and the hidden state builds up over the course of the text. The final hidden state then is supposed to encode something sensible about the text. We take the final hidden layer and pass it into a linear layer to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11839d-3420-443f-ae66-f2da9bbb69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-text",
   "language": "python",
   "name": "pytorch-text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
