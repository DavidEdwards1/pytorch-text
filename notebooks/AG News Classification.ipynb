{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a8f63e-b61d-4d19-b68b-bac6b8f78040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import uuid\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc82c6-0c86-48c2-b053-055d34b929d7",
   "metadata": {},
   "source": [
    "This notebook is based on the PyTorch tutorial [Text classification with the torchtext library](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) with my own notes supplementing what can be found there. We also add TensorBoard logging to model for visualisation of the embedding layer and tracking of model training.\n",
    "\n",
    "We use the [AG News Dataset](https://paperswithcode.com/dataset/ag-news). Each sample from the iterator is a tuple of `(label, text)` where `text` is an amalgamation of the `title`, `source` and `description` fields that are defined in the [original source](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html).\n",
    "\n",
    "The dataset is a classification dataset and the target labels are:  \n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tec\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "We want to apply a simple text processing pipeline to each sample in our data:\n",
    " 1. Tokenise: Split our inputs in to individual words\n",
    " 2. Encode each word as integer (its index in our vocabulary)\n",
    " \n",
    "We can use any tokeniser we want, but for simplicity just use [`torchtext`'s provided `basic_english` tokeniser](https://pytorch.org/text/stable/data_utils.html). This means that punctuation gets its own token for now.\n",
    "\n",
    "PyTorch leaves the process of counting the token occurances to you (using `collections.Counter`) which you then pass into a [`torchtext.vocab.Vocab`](https://pytorch.org/text/stable/vocab.html) that handles the encoding, can also do things like fix total size, a minimum occurance frequency etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972f78cc-14fb-4ba8-8b8a-b227f9665fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "counter = collections.Counter()\n",
    "for (label, text) in train_iter:\n",
    "    counter.update(tokenizer(text))\n",
    "    \n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88271d4e-baed-4f51-ace7-9da56e6f2480",
   "metadata": {},
   "source": [
    "Now we define the functions that we want to apply to each line of data and use them in a `collate_fn` that we will apply to an entire batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19194eb3-4440-4c31-b0c0-aa7615d1bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class TextPipeline:\n",
    "    def __init__(self, vocab, tokenizer):\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        return [self.vocab[token] for token in self.tokenizer(text)]\n",
    "\n",
    "class LabelPipeline:\n",
    "    def __call__(self, label):\n",
    "        return int(label) - 1\n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, text_pipeline, label_pipeline):\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare batch of data to be used as input to torch model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          labels: a torch.tensor of integer encoded labels. Has shape (batch_size)\n",
    "          texts: a torch.tensor of integer encoded text sequences. Encoded using text_pipeline.\n",
    "              Each example is concatenated together into a flat 1D tensor. The start of each\n",
    "              example is recorded in offsets. Has shape (n_tokens_in_batch)\n",
    "          offsets: a torch.tensor of the index of the start of each example.\n",
    "              Has shape (batch_size)\n",
    "        \"\"\"\n",
    "        labels, texts, offsets = [], [], [0]\n",
    "\n",
    "        for (label, text) in batch:\n",
    "            labels.append(\n",
    "                self.label_pipeline(label)\n",
    "            )\n",
    "            processed_text = torch.tensor(\n",
    "                self.text_pipeline(text),\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "            texts.append(processed_text)\n",
    "            offsets.append(processed_text.size(0)) # length of processed text\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # starting index of each example\n",
    "        texts = torch.cat(texts) # we can treat this differently as it is a list of tensors\n",
    "\n",
    "        return labels.to(device), texts.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b2036-280a-471b-a4a0-e6a76c67486c",
   "metadata": {},
   "source": [
    "In PyTorch, a [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) wraps a Dataset as an iterable and allows for batching, sampling, shuffling and multiprocess data loading. The AG News dataset is an _iterable_ dataset and so we can't use sampling or shuffle (since in principle these would be dealt with by the iterator).\n",
    "\n",
    "The above classes could then be used as follows in `DataLoader`s during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d940ca5-d837-49e3-99ca-420ae7c320a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "collator = Collator(\n",
    "    TextPipeline(vocab, tokenizer),\n",
    "    LabelPipeline()\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb718e7-117e-4923-b0f7-84131dc2596a",
   "metadata": {},
   "source": [
    "# Predictive Model\n",
    "\n",
    "## Definition\n",
    "The model is an embedding layer (actually a [`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag) followed by 2 [Linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (ie a simple two layer MLP the ouptut of the embedding layer). This is a slight extension of the Tutorial that this Notebook is based on.\n",
    "\n",
    "## Embedding Layer\n",
    "The embedding layer calculates the mean of the embeddings of each text we send it (in this case the text is the bag). Roughly we can imagine this as:\n",
    "  1. Set `i = 0`\n",
    "  2. Take the slice `texts[offset[i]:[offset[i+1]]`\n",
    "  3. Calculate the word vector for each token in the slice\n",
    "  4. Calculate the mean of all the word vectors in the slice\n",
    "\n",
    "This means that every sample get converted into a single tensor (the mean of the tensors for each individual token in the input sample) regardless of the length of the input text avoiding the mean for padding or truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50997604-9be3-4478-b496-534573004bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_neurons, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, hidden_neurons[0], sparse=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_neurons[0], hidden_neurons[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons[1], num_classes),\n",
    "        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc[0].weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc[0].bias.data.zero_()\n",
    "        self.fc[2].weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc[2].bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0bbef-7f7d-469e-9f51-30888caad846",
   "metadata": {},
   "source": [
    "# Define Training Loop and Evaluation Function\n",
    "\n",
    "This requires some knowledge of the peculiarities of PyTorch, including:\n",
    " - PyTorch basically doesn't do anything unless you tell it to. This means that you have to explicitly construct each step of your training loops e.g. forward pass, backward pass, calculating loss etc\n",
    " - [You have to set the \"mode\" of the model](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch), e.g. during training you should call `model.train()`. This means that layers that behave differently during training and evaluation (for example dropout layers) will do the right thing.\n",
    " - [By default the optimizer accumlates gradients on each call of `loss.backward()`](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch). This means that we need to call `optimizer.zero_grad()` as the first stage of each training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a676676-b209-4cd7-be69-5e5d42464249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_correct, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predited_label = model(text, offsets)\n",
    "        \n",
    "        loss = loss_fn(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_correct += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_correct/total_count))\n",
    "            total_correct, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_correct, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            total_correct += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_correct / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3a7c3-0a22-4163-8904-01b5f2343fda",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79b628-bb58-48c9-b7be-28fd22a1f0c3",
   "metadata": {},
   "source": [
    "Something exciting about training the model here\n",
    "\n",
    "At the end of training we will visualise the embedding layer in two ways using tensorboard.\n",
    " 1. We will look at the embeddings of the individual words\n",
    " 2. And the embeddings of the full texts. We will label these by the genre of the article\n",
    "The hope is that since we are attempting to learn embeddings useful for the categorisation problem at hand we can see that the embedding vector for the full texts cluster with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce2f89a9-b283-46e5-a263-b51ccacc4f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/  891 batches | accuracy    0.626\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  9.93s | valid accuracy    0.830 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/  891 batches | accuracy    0.854\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  9.75s | valid accuracy    0.867 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/  891 batches | accuracy    0.888\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  9.55s | valid accuracy    0.873 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/  891 batches | accuracy    0.903\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  9.65s | valid accuracy    0.887 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/  891 batches | accuracy    0.914\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 10.17s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/  891 batches | accuracy    0.920\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  9.76s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/  891 batches | accuracy    0.927\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  9.74s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/  891 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  9.57s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/  891 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 10.23s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/  891 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 10.05s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "hidden_neurons = [64, 64]\n",
    "\n",
    "model = TextClassificationModel(vocab_size, hidden_neurons, num_class).to(device)\n",
    "\n",
    "experiment_id = uuid.uuid4().hex\n",
    "writer = SummaryWriter(f'runs/ag_news/experiment_{experiment_id}')\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 128# batch size for training\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# this changes the dataset from being an iterable to a map (ie accessible by index)\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "\n",
    "# create 95% train/val split with test heldout\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset,\n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")\n",
    "\n",
    "writer.add_graph(\n",
    "    model,\n",
    "    input_to_model=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )(train_dataset[:5])[1:]\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "    writer.add_scalar(\"Accuracy/validation\", accu_val, epoch)\n",
    "    \n",
    "\n",
    "# at the end of training record the embedding of individual tokens\n",
    "writer.add_embedding(\n",
    "    model.embedding.weight,\n",
    "    metadata=list(vocab.stoi.keys()),\n",
    "    tag=\"all_tokens\"\n",
    ")\n",
    "    \n",
    "    \n",
    "writer.flush()    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac489d-5aaa-4455-939b-135fd22b57e8",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Finally, evaluate the model performance on the heldout test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3462ad79-2b67-4c16-ac18-86a8a28c6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.910\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f93e9-919a-4ea0-a0c5-2a0763cf7124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-text",
   "language": "python",
   "name": "pytorch-text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
