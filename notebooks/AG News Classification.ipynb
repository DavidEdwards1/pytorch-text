{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a8f63e-b61d-4d19-b68b-bac6b8f78040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc82c6-0c86-48c2-b053-055d34b929d7",
   "metadata": {},
   "source": [
    "Going to use built-in training datasets and compare against known results. Should probably link off to the pytorch datasets docs here.\n",
    "\n",
    "Here we use the [AG News Dataset](https://paperswithcode.com/dataset/ag-news).\n",
    "\n",
    "Each sample from the iterator is a tuple of `(label, text)` where `text` is an amalgamation of the `title`, `source` and `description` fields that are defined in the [original source](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html).\n",
    "\n",
    "The labels are:  \n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tec\n",
    "\n",
    "In PyTorch, a [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) wraps a Dataset as an iterable and allows for batching, sampling, shuffling and multiprocess data loading. The AG News dataset is an _iterable_ dataset and so we can't use sampling or shuffle (since in principle these would be dealt with by the iterator).\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "We want to apply a simple text processing pipeline to each sample in our data:\n",
    " 1. Tokenise: Split our inputs in to individual words\n",
    " 2. Encode each word as integer (its index in our vocabulary)\n",
    " \n",
    "We can use any tokeniser we want, but for simplicity just use [`torchtext`'s provided `basic_english` tokeniser](https://pytorch.org/text/stable/data_utils.html). This means that punctuation gets its own token for now.\n",
    "\n",
    "PyTorch leaves the process of counting the token occurances to you (using `collections.Counter`) which you then pass into a [`torchtext.vocab.Vocab`](https://pytorch.org/text/stable/vocab.html) that handles the encoding, can also do things like fix total size, a minimum occurance frequency etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972f78cc-14fb-4ba8-8b8a-b227f9665fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.csv: 29.5MB [00:00, 33.8MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "counter = collections.Counter()\n",
    "for (label, text) in train_iter:\n",
    "    counter.update(tokenizer(text))\n",
    "    \n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88271d4e-baed-4f51-ace7-9da56e6f2480",
   "metadata": {},
   "source": [
    "Now we define the functions that we want to apply to each line of data and use them in a `collate_fn` that we will apply to an entire batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19194eb3-4440-4c31-b0c0-aa7615d1bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class TextPipeline:\n",
    "    def __init__(self, vocab, tokenizer):\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        return [self.vocab[token] for token in self.tokenizer(text)]\n",
    "\n",
    "class LabelPipeline:\n",
    "    def __call__(self, label):\n",
    "        return int(label) - 1\n",
    "    \n",
    "class Collator:\n",
    "    def __init__(self, text_pipeline, label_pipeline):\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare batch of data to be used as input to torch model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          labels: a torch.tensor of integer encoded labels. Has shape (batch_size)\n",
    "          texts: a torch.tensor of integer encoded text sequences. Encoded using text_pipeline.\n",
    "              Each example is concatenated together into a flat 1D tensor. The start of each\n",
    "              example is recorded in offsets. Has shape (n_tokens_in_batch)\n",
    "          offsets: a torch.tensor of the index of the start of each example.\n",
    "              Has shape (batch_size)\n",
    "        \"\"\"\n",
    "        labels, texts, offsets = [], [], [0]\n",
    "\n",
    "        for (label, text) in batch:\n",
    "            labels.append(\n",
    "                self.label_pipeline(label)\n",
    "            )\n",
    "            processed_text = torch.tensor(\n",
    "                self.text_pipeline(text),\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "            texts.append(processed_text)\n",
    "            offsets.append(processed_text.size(0)) # length of processed text\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # starting index of each example\n",
    "        texts = torch.cat(texts) # we can treat this differently as it is a list of tensors\n",
    "\n",
    "        return labels.to(device), texts.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d940ca5-d837-49e3-99ca-420ae7c320a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e64a94-2dd5-4f9f-bc0d-0eebef7e5e3e",
   "metadata": {},
   "source": [
    "# Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb718e7-117e-4923-b0f7-84131dc2596a",
   "metadata": {},
   "source": [
    "Define the model. The model is an embedding layer (actually a torch `nn.EmbeddingBag`) followed by a [Linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (ie essentially a Logistic Regression on the ouptut of the embedding layer).\n",
    "\n",
    "The embedding layer calculates the mean of the embeddings of each text we send it (in this case the text is the bag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50997604-9be3-4478-b496-534573004bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0bbef-7f7d-469e-9f51-30888caad846",
   "metadata": {},
   "source": [
    "# Define Training Loop and Evaluation Function\n",
    "\n",
    "This requires some knowledge of the peculiarities of PyTorch, including:\n",
    " - PyTorch basically doesn't do anything unless you tell it to. This means that you have to explicitly construct each step of your training loops e.g. forward pass, backward pass, calculating loss etc\n",
    " - [You have to set the \"mode\" of the model](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch), e.g. during training you should call `model.train()`. This means that layers that behave differently during training and evaluation (for example dropout layers) will do the right thing.\n",
    " - [By default the optimizer accumlates gradients on each call of `loss.backward()`](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch). This means that we need to call `optimizer.zero_grad()` as the first stage of each training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a676676-b209-4cd7-be69-5e5d42464249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_correct, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predited_label = model(text, offsets)\n",
    "        \n",
    "        loss = loss_fn(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_correct += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_correct/total_count))\n",
    "            total_correct, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_correct, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            total_correct += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_correct / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3a7c3-0a22-4163-8904-01b5f2343fda",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91a6187-578b-4976-86b7-08dc8b82d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "emsize = 64\n",
    "\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2f89a9-b283-46e5-a263-b51ccacc4f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test.csv: 1.86MB [00:00, 17.0MB/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.673\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.853\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.874\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 11.12s | valid accuracy    0.883 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.898\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.897\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 10.68s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.917\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.911\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 10.67s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.923\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.923\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.923\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 10.97s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.928\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.929\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 10.77s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.942\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 10.79s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.944\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 11.03s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.947\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 10.84s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 10.66s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.945\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 11.09s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# this changes the dataset from being an iterable to a map (ie accessible by index)\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "\n",
    "# create 95% train/val split with test heldout\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset,\n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=Collator(\n",
    "        TextPipeline(vocab, tokenizer),\n",
    "        LabelPipeline()\n",
    "    )\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3462ad79-2b67-4c16-ac18-86a8a28c6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.906\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
